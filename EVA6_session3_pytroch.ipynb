{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA6_session3_pytroch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RohinSequeira/EVA6_Session3_Pytorch/blob/main/EVA6_session3_pytroch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PxCwSfZVtvb"
      },
      "source": [
        "from __future__ import print_function #Allowing the interpreter to use old version of python with features of new verion, in this case, print\n",
        "import torch # importing torch library\n",
        "import torch.nn as nn #torch.nn provides basic building blocks for graphs, as per pytorch documentation\n",
        "import torch.nn.functional as F # importing all the functions in torch.nn library\n",
        "import torch.optim as optim # contains various optimization algorithms\n",
        "from torchvision import datasets, transforms #contains popular datasets and commom image transformations for computer vision\n",
        "from torchsummary import summary #provides clean and clear visual summary of the model\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch import Tensor"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IQsM3DNVwbk"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ae0nY3M3WQIM"
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      \"\"\"\n",
        "      In this constuctor, we define parameters, representing the different layers that will make\n",
        "      up the Neural Network\n",
        "      \"\"\"\n",
        "      super(Net, self).__init__()\n",
        "\n",
        "      self.conv1 = nn.Conv2d(1, 32, 3, padding=1) #input -? OUtput? RF\n",
        "      self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "      self.pool1 = nn.MaxPool2d(2, 2)\n",
        "      self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "      self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "      self.pool2 = nn.MaxPool2d(2, 2)\n",
        "      self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "      self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "      self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "\n",
        "      # First fully connected layer\n",
        "      self.fc1 = nn.Linear(11, 5, bias=False)\n",
        "      self.fc2 = nn.Linear(5, 1, bias=False)\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "      \"\"\"\n",
        "      To compute output Tensors from the input Tensors. Sequence of layers are defined here.\n",
        "      \"\"\"\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x1)))))\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        x = self.conv7(x)\n",
        "        x = x.view(-1, 10)\n",
        "\n",
        "        n1 = torch.argmax(x,dim=1).reshape(-1,1)\n",
        "        n2 = torch.cat((n1,x2),dim=1)\n",
        "        n  = self.fc2(self.fc1(n2))\n",
        "\n",
        "        return F.log_softmax(x), n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snRlwIIPWUKK"
      },
      "source": [
        "my_nn = Net().to(device)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpFRh3-2_AHa"
      },
      "source": [
        "# Download and load MNIST datasets for train and test of Convolutional Network. \n",
        "# torch.utils.data.DataLoader class provided by pytorch is used for this purpose.\n",
        "torch.manual_seed(1)\n",
        "batch_size = 64\n",
        "\n",
        "train_data = datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ]))\n",
        "\n",
        "test_data = datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ]))\n",
        "train_random_input = torch.randint(0,10,(60000,))\n",
        "train_target2 = torch.add(train_random_input, train_data.targets)\n",
        "\n",
        "test_random_input = torch.randint(0,10,(10000,))\n",
        "test_target2 = torch.add(test_random_input, test_data.targets)\n",
        "train_one_hot = torch.nn.functional.one_hot(train_random_input)\n",
        "test_one_hot = torch.nn.functional.one_hot(test_random_input)\n",
        "\n",
        "kwargs = {'num_workers': 2, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(train_data,\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(test_data,\n",
        "    batch_size=batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "random_train_dataset = TensorDataset(train_one_hot, train_target2)\n",
        "random_train_loader = torch.utils.data.DataLoader(random_train_dataset, batch_size=batch_size, shuffle=False, **kwargs)\n",
        "\n",
        "random_test_dataset = TensorDataset(test_one_hot, test_target2)\n",
        "random_test_loader = torch.utils.data.DataLoader(random_test_dataset, batch_size=batch_size, shuffle=False, **kwargs)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHNrHASmWvaF"
      },
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "optimizer = torch.optim.SGD(my_nn.parameters(), lr=learning_rate) "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLWrh1EsW4RN"
      },
      "source": [
        "def train(model, device, train_loader1, train_loader2, optimizer, epoch):\n",
        "  \"\"\"\n",
        "  Define Train functionality.\n",
        "  \"\"\"\n",
        "    model.train()\n",
        "    for batch_idx, (dt_1, dt_2) in enumerate(zip(train_loader1, train_loader2)):\n",
        "        data1, data2, target1,  target2 = dt_1[0].to(device), dt_2[0].float().to(device), dt_1[1].to(device), dt_2[1].type(torch.LongTensor).to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output1, output2 = model(data1, data2)\n",
        "        criterion1 = nn.NLLLoss()\n",
        "        loss1 = criterion1(output1, target1)\n",
        "        criterion2 = nn.MSELoss()\n",
        "        loss2 = criterion2(output2, target2.float())\n",
        "        loss = loss1 + loss2\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 50 == 0:\n",
        "          print(f'loss of image {loss1}')\n",
        "          print(f'loss of summed value {loss2}')\n",
        "          print(f'loss={loss.item()} batch_id={batch_idx}')\n",
        "    return 0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX3Q2k7ZXAYf",
        "outputId": "ff8c28ff-81f2-48bf-9033-80197c2c3a56"
      },
      "source": [
        "#Trigger Model Train and Test in epochs with for loop.\n",
        "optimizer.zero_grad()\n",
        "for epoch in range(1, 6):\n",
        "    # Train the model using all the defined objects\n",
        "    predicted = train(my_nn, device, train_loader, random_train_loader, optimizer, epoch)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss of image 2.302597761154175\n",
            "loss of summed value 93.62191009521484\n",
            "loss=95.92450714111328 batch_id=0\n",
            "loss of image 2.3043293952941895\n",
            "loss of summed value 31.53976821899414\n",
            "loss=33.84409713745117 batch_id=50\n",
            "loss of image 2.3004634380340576\n",
            "loss of summed value 14.67499828338623\n",
            "loss=16.975461959838867 batch_id=100\n",
            "loss of image 2.297588348388672\n",
            "loss of summed value 15.166685104370117\n",
            "loss=17.46427345275879 batch_id=150\n",
            "loss of image 2.297001361846924\n",
            "loss of summed value 15.455636978149414\n",
            "loss=17.75263786315918 batch_id=200\n",
            "loss of image 2.297227621078491\n",
            "loss of summed value 14.183305740356445\n",
            "loss=16.480533599853516 batch_id=250\n",
            "loss of image 2.2977945804595947\n",
            "loss of summed value 19.940078735351562\n",
            "loss=22.237873077392578 batch_id=300\n",
            "loss of image 2.2974205017089844\n",
            "loss of summed value 13.721067428588867\n",
            "loss=16.01848793029785 batch_id=350\n",
            "loss of image 2.296058177947998\n",
            "loss of summed value 20.344310760498047\n",
            "loss=22.640369415283203 batch_id=400\n",
            "loss of image 2.29622745513916\n",
            "loss of summed value 16.191539764404297\n",
            "loss=18.48776626586914 batch_id=450\n",
            "loss of image 2.295363187789917\n",
            "loss of summed value 25.996896743774414\n",
            "loss=28.292259216308594 batch_id=500\n",
            "loss of image 2.2920937538146973\n",
            "loss of summed value 22.669143676757812\n",
            "loss=24.96123695373535 batch_id=550\n",
            "loss of image 2.2910711765289307\n",
            "loss of summed value 41.00886535644531\n",
            "loss=43.2999382019043 batch_id=600\n",
            "loss of image 2.2867114543914795\n",
            "loss of summed value 40.661354064941406\n",
            "loss=42.94806671142578 batch_id=650\n",
            "loss of image 2.2875819206237793\n",
            "loss of summed value 46.735599517822266\n",
            "loss=49.0231819152832 batch_id=700\n",
            "loss of image 2.286168336868286\n",
            "loss of summed value 45.17068099975586\n",
            "loss=47.45684814453125 batch_id=750\n",
            "loss of image 2.2851436138153076\n",
            "loss of summed value 45.40351867675781\n",
            "loss=47.688663482666016 batch_id=800\n",
            "loss of image 2.2832016944885254\n",
            "loss of summed value 38.06749725341797\n",
            "loss=40.35070037841797 batch_id=850\n",
            "loss of image 2.2895827293395996\n",
            "loss of summed value 40.44739532470703\n",
            "loss=42.736976623535156 batch_id=900\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss of image 2.279109239578247\n",
            "loss of summed value 41.14697265625\n",
            "loss=43.426082611083984 batch_id=0\n",
            "loss of image 2.272397994995117\n",
            "loss of summed value 40.81637191772461\n",
            "loss=43.088768005371094 batch_id=50\n",
            "loss of image 2.270951509475708\n",
            "loss of summed value 33.34664535522461\n",
            "loss=35.61759567260742 batch_id=100\n",
            "loss of image 2.2684812545776367\n",
            "loss of summed value 40.48430633544922\n",
            "loss=42.75278854370117 batch_id=150\n",
            "loss of image 2.26603627204895\n",
            "loss of summed value 35.03494644165039\n",
            "loss=37.30098342895508 batch_id=200\n",
            "loss of image 2.2586801052093506\n",
            "loss of summed value 35.84098815917969\n",
            "loss=38.099666595458984 batch_id=250\n",
            "loss of image 2.2561652660369873\n",
            "loss of summed value 36.338592529296875\n",
            "loss=38.594757080078125 batch_id=300\n",
            "loss of image 2.2557241916656494\n",
            "loss of summed value 25.797340393066406\n",
            "loss=28.053064346313477 batch_id=350\n",
            "loss of image 2.2309494018554688\n",
            "loss of summed value 23.618003845214844\n",
            "loss=25.848953247070312 batch_id=400\n",
            "loss of image 2.216728448867798\n",
            "loss of summed value 25.350303649902344\n",
            "loss=27.567031860351562 batch_id=450\n",
            "loss of image 2.2142834663391113\n",
            "loss of summed value 24.937400817871094\n",
            "loss=27.151683807373047 batch_id=500\n",
            "loss of image 2.2403321266174316\n",
            "loss of summed value 19.961727142333984\n",
            "loss=22.202058792114258 batch_id=550\n",
            "loss of image 2.1948089599609375\n",
            "loss of summed value 29.867591857910156\n",
            "loss=32.062400817871094 batch_id=600\n",
            "loss of image 2.1105501651763916\n",
            "loss of summed value 26.492450714111328\n",
            "loss=28.60300064086914 batch_id=650\n",
            "loss of image 2.113619327545166\n",
            "loss of summed value 26.558643341064453\n",
            "loss=28.67226219177246 batch_id=700\n",
            "loss of image 2.0760114192962646\n",
            "loss of summed value 23.35061264038086\n",
            "loss=25.426624298095703 batch_id=750\n",
            "loss of image 1.9447376728057861\n",
            "loss of summed value 23.47416877746582\n",
            "loss=25.418907165527344 batch_id=800\n",
            "loss of image 1.853110671043396\n",
            "loss of summed value 17.65053939819336\n",
            "loss=19.503650665283203 batch_id=850\n",
            "loss of image 1.4763137102127075\n",
            "loss of summed value 20.136642456054688\n",
            "loss=21.612957000732422 batch_id=900\n",
            "loss of image 1.3855699300765991\n",
            "loss of summed value 15.768786430358887\n",
            "loss=17.154356002807617 batch_id=0\n",
            "loss of image 1.134521484375\n",
            "loss of summed value 15.779598236083984\n",
            "loss=16.914119720458984 batch_id=50\n",
            "loss of image 0.7759245038032532\n",
            "loss of summed value 14.999897003173828\n",
            "loss=15.775821685791016 batch_id=100\n",
            "loss of image 1.0237785577774048\n",
            "loss of summed value 16.796606063842773\n",
            "loss=17.820384979248047 batch_id=150\n",
            "loss of image 0.6876305341720581\n",
            "loss of summed value 16.24462127685547\n",
            "loss=16.9322509765625 batch_id=200\n",
            "loss of image 0.6748896837234497\n",
            "loss of summed value 14.81193733215332\n",
            "loss=15.48682689666748 batch_id=250\n",
            "loss of image 0.6904906630516052\n",
            "loss of summed value 20.142255783081055\n",
            "loss=20.832746505737305 batch_id=300\n",
            "loss of image 0.6151286363601685\n",
            "loss of summed value 14.640752792358398\n",
            "loss=15.255881309509277 batch_id=350\n",
            "loss of image 0.5060675740242004\n",
            "loss of summed value 10.84346866607666\n",
            "loss=11.349535942077637 batch_id=400\n",
            "loss of image 0.5505133867263794\n",
            "loss of summed value 15.74710464477539\n",
            "loss=16.297618865966797 batch_id=450\n",
            "loss of image 0.3956591784954071\n",
            "loss of summed value 17.43777847290039\n",
            "loss=17.833436965942383 batch_id=500\n",
            "loss of image 0.32610684633255005\n",
            "loss of summed value 15.602384567260742\n",
            "loss=15.928491592407227 batch_id=550\n",
            "loss of image 0.49887815117836\n",
            "loss of summed value 24.087116241455078\n",
            "loss=24.585994720458984 batch_id=600\n",
            "loss of image 0.6319950819015503\n",
            "loss of summed value 21.469486236572266\n",
            "loss=22.10148048400879 batch_id=650\n",
            "loss of image 0.49070537090301514\n",
            "loss of summed value 19.77027130126953\n",
            "loss=20.260976791381836 batch_id=700\n",
            "loss of image 0.8074370622634888\n",
            "loss of summed value 16.571353912353516\n",
            "loss=17.37879180908203 batch_id=750\n",
            "loss of image 0.37436044216156006\n",
            "loss of summed value 17.70309829711914\n",
            "loss=18.07745933532715 batch_id=800\n",
            "loss of image 0.7669661641120911\n",
            "loss of summed value 15.115853309631348\n",
            "loss=15.882819175720215 batch_id=850\n",
            "loss of image 0.42317086458206177\n",
            "loss of summed value 17.690967559814453\n",
            "loss=18.114137649536133 batch_id=900\n",
            "loss of image 0.5127953886985779\n",
            "loss of summed value 14.145283699035645\n",
            "loss=14.658079147338867 batch_id=0\n",
            "loss of image 0.3045237064361572\n",
            "loss of summed value 13.771744728088379\n",
            "loss=14.076268196105957 batch_id=50\n",
            "loss of image 0.5449852347373962\n",
            "loss of summed value 14.795700073242188\n",
            "loss=15.34068489074707 batch_id=100\n",
            "loss of image 0.27273473143577576\n",
            "loss of summed value 16.591121673583984\n",
            "loss=16.86385726928711 batch_id=150\n",
            "loss of image 0.592366635799408\n",
            "loss of summed value 15.504295349121094\n",
            "loss=16.096662521362305 batch_id=200\n",
            "loss of image 0.396568238735199\n",
            "loss of summed value 14.064826011657715\n",
            "loss=14.461394309997559 batch_id=250\n",
            "loss of image 0.3445659279823303\n",
            "loss of summed value 19.43902015686035\n",
            "loss=19.783586502075195 batch_id=300\n",
            "loss of image 0.426662415266037\n",
            "loss of summed value 14.080277442932129\n",
            "loss=14.506939888000488 batch_id=350\n",
            "loss of image 0.22598610818386078\n",
            "loss of summed value 10.791059494018555\n",
            "loss=11.017045974731445 batch_id=400\n",
            "loss of image 0.49466148018836975\n",
            "loss of summed value 16.558727264404297\n",
            "loss=17.053388595581055 batch_id=450\n",
            "loss of image 0.5206201672554016\n",
            "loss of summed value 17.49166488647461\n",
            "loss=18.012285232543945 batch_id=500\n",
            "loss of image 0.30255597829818726\n",
            "loss of summed value 15.767839431762695\n",
            "loss=16.07039451599121 batch_id=550\n",
            "loss of image 0.3647341728210449\n",
            "loss of summed value 24.367063522338867\n",
            "loss=24.73179817199707 batch_id=600\n",
            "loss of image 0.5114085674285889\n",
            "loss of summed value 21.631893157958984\n",
            "loss=22.143301010131836 batch_id=650\n",
            "loss of image 0.33224740624427795\n",
            "loss of summed value 19.354122161865234\n",
            "loss=19.686368942260742 batch_id=700\n",
            "loss of image 0.24925005435943604\n",
            "loss of summed value 16.534006118774414\n",
            "loss=16.78325653076172 batch_id=750\n",
            "loss of image 0.2804432511329651\n",
            "loss of summed value 17.508554458618164\n",
            "loss=17.788997650146484 batch_id=800\n",
            "loss of image 0.41498899459838867\n",
            "loss of summed value 15.188522338867188\n",
            "loss=15.603511810302734 batch_id=850\n",
            "loss of image 0.3113726079463959\n",
            "loss of summed value 17.803110122680664\n",
            "loss=18.114482879638672 batch_id=900\n",
            "loss of image 0.33842483162879944\n",
            "loss of summed value 14.106646537780762\n",
            "loss=14.44507122039795 batch_id=0\n",
            "loss of image 0.4477616548538208\n",
            "loss of summed value 13.73202133178711\n",
            "loss=14.17978286743164 batch_id=50\n",
            "loss of image 0.1439470499753952\n",
            "loss of summed value 14.895669937133789\n",
            "loss=15.039616584777832 batch_id=100\n",
            "loss of image 0.3395044803619385\n",
            "loss of summed value 15.331802368164062\n",
            "loss=15.671306610107422 batch_id=150\n",
            "loss of image 0.276328980922699\n",
            "loss of summed value 15.46506118774414\n",
            "loss=15.741390228271484 batch_id=200\n",
            "loss of image 0.3253030478954315\n",
            "loss of summed value 14.065982818603516\n",
            "loss=14.39128589630127 batch_id=250\n",
            "loss of image 0.4185377359390259\n",
            "loss of summed value 19.40801239013672\n",
            "loss=19.826549530029297 batch_id=300\n",
            "loss of image 0.4106398820877075\n",
            "loss of summed value 14.127311706542969\n",
            "loss=14.537951469421387 batch_id=350\n",
            "loss of image 0.49687233567237854\n",
            "loss of summed value 10.63673210144043\n",
            "loss=11.133604049682617 batch_id=400\n",
            "loss of image 0.30244219303131104\n",
            "loss of summed value 15.90298843383789\n",
            "loss=16.20543098449707 batch_id=450\n",
            "loss of image 0.23754428327083588\n",
            "loss of summed value 17.302318572998047\n",
            "loss=17.53986358642578 batch_id=500\n",
            "loss of image 0.32351937890052795\n",
            "loss of summed value 15.758686065673828\n",
            "loss=16.082204818725586 batch_id=550\n",
            "loss of image 0.10927920788526535\n",
            "loss of summed value 23.301170349121094\n",
            "loss=23.410449981689453 batch_id=600\n",
            "loss of image 0.33517175912857056\n",
            "loss of summed value 21.580347061157227\n",
            "loss=21.91551971435547 batch_id=650\n",
            "loss of image 0.3946760892868042\n",
            "loss of summed value 19.370845794677734\n",
            "loss=19.765522003173828 batch_id=700\n",
            "loss of image 0.32100197672843933\n",
            "loss of summed value 16.739484786987305\n",
            "loss=17.06048583984375 batch_id=750\n",
            "loss of image 0.3429878354072571\n",
            "loss of summed value 17.573333740234375\n",
            "loss=17.91632080078125 batch_id=800\n",
            "loss of image 0.3759331703186035\n",
            "loss of summed value 15.16031265258789\n",
            "loss=15.536245346069336 batch_id=850\n",
            "loss of image 0.3530414402484894\n",
            "loss of summed value 17.883960723876953\n",
            "loss=18.237001419067383 batch_id=900\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL_Ywob-ze0H"
      },
      "source": [
        "def test(model, device, test_loader1, test_loader2):\n",
        "  \"\"\"\n",
        "  # Define Test functionality, along with measuring accuracy of prediction.\n",
        "  \"\"\"\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    predicted_sum=torch.FloatTensor().to(device)\n",
        "    for batch_idx, (dt_1, dt_2) in enumerate(zip(test_loader1, test_loader2)):\n",
        "        data1, data2,target1, target2 = dt_1[0].to(device), dt_2[0].float().to(device), dt_1[1].to(device), dt_2[1].to(device)\n",
        "        output1, output2 = model(data1, data2)\n",
        "        criterion1 = nn.NLLLoss() #Loss function for MNIST\n",
        "        loss1 = criterion1(output1, target1)\n",
        "        criterion2 = nn.MSELoss() #Loss function for sum of digits\n",
        "        loss2 = criterion2(output2, target2.float()) #Cumulative loss\n",
        "        test_loss += float(loss1 + loss2)\n",
        "        pred = output1.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "        correct += pred.eq(target1.view_as(pred)).sum().item()\n",
        "        predicted_sum=torch.cat((predicted_sum,output2.detach().reshape(-1)))\n",
        "        \n",
        "    \n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    return predicted_sum"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEIF3QVCm5nG"
      },
      "source": [
        "del train_data, train_random_input, train_target2"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LybIIyN6di1P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e47c3a1e-6d44-48c0-989c-21704f71f91c"
      },
      "source": [
        "pred = test(my_nn, device, test_loader, random_test_loader)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Test set: Average loss: 0.2655, Accuracy: 9180/10000 (92%)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EcWSNaFEd4nu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6e4cfba-70f2-4b8d-b771-af90b0f66836"
      },
      "source": [
        "#Measure R2 score as metric for final data sum. Output sucks, we know.\n",
        "from sklearn.metrics import r2_score\n",
        "r2_score(test_target2, pred.cpu())\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06546127184631612"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXXKk6Iz0C3M"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}